{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3be54a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT SENTENCE: His son looks like a donkey\n",
      "\n",
      "CATEGORY: Dissing\n",
      "TYPE: Behavioral-based\n",
      "SUGGESTED ALTERNATIVES:\n",
      "1. His son has unique features.\n",
      "2. I don't think his son's appearance is conventionally attractive.\n",
      "\n",
      "HARMFUL CONTENT IDENTIFICATION:\n",
      "⚠️ donkey\n",
      "\n",
      "TOTAL WORDS: 6\n",
      "FLAGGED PERCENTAGE: 17% (1 Harmful Word / 6 Total Words)\n",
      "\n",
      "REASON:The sentence uses a derogatory term (donkey) to insult someone's appearance, thus fitting the Dissing category of cyberbullying.  It is behavioral-based as it focuses on the action of insulting rather than targeting a specific identity.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Load the API key from .env file\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# Configure the API key for Google Generative AI\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "# Initialize the Generative Model\n",
    "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")\n",
    "\n",
    "# Function to count words in the input text\n",
    "def count_words(text):\n",
    "    return len(text.split())\n",
    "\n",
    "# Define the prompt function for analyzing cyberbullying\n",
    "def analyze_cyberbullying(text):\n",
    "    total_words = count_words(text)\n",
    "    prompt = (\n",
    "        \"Classify the given sentence into one of the following cyberbullying CATEGORIES: Race/Ethnicity, Gender/Sexual, Religion, Harassment, Flaming/Trolling, Dissing;\"\n",
    "        \"IF the CATEGORY is Race/Ethnicity, Gender/Sexual or Religion, then write the TYPE as Identity-based;\"\n",
    "        \"IF the CATEGORY is Harassment, Flaming/Trolling, or Dissing, then write the TYPE as Behavioral-based;\"\n",
    "        \"Write CATEGORY first, then SUB CATEGORY on separate lines;\"\n",
    "        \"If the sentence is not cyberbullying, respond with 'Not cyberbullying';\"\n",
    "        \"SUGGESTED ALTERNATIVES: Suggest only 2 neutral/safer ways to express the sentence - no yapping;\"\n",
    "        \"HARMFUL CONTENT IDENTIFICATION: Display only the individual harmful words from the sentence as a list, each marked with ⚠️. Do not include phrases, explanations, or additional text;\"\n",
    "        f\"TOTAL WORDS: {total_words};\"\n",
    "        \"FLAGGED PERCENTAGE: Display the percentage along with the breakdown like this: 20% (2 Harmful Words / 10 Total Words);\"\n",
    "        \"REASON: Briefly justify why the message was flagged and its cyberbullying category - no yapping;\"\n",
    "        f\"Sentence: {text}\"\n",
    "    )\n",
    "\n",
    "    # Get the response from the AI model\n",
    "    response = model.generate_content([prompt])\n",
    "    cleaned_output = re.sub(r'\\*\\*|\\*|## |\"', '', response.text)\n",
    "    return cleaned_output.strip()\n",
    "\n",
    "# Main function to handle user input and output\n",
    "def main():\n",
    "    # Get user input\n",
    "    user_input = input(\"Enter a sentence to classify: \")\n",
    "    \n",
    "    # Call the analyze_cyberbullying function\n",
    "    result = analyze_cyberbullying(user_input)\n",
    "\n",
    "    # Combine user input and result for display\n",
    "    final_output = f\"INPUT SENTENCE: {user_input}\\n\\n{result}\"\n",
    "\n",
    "    # Print the result\n",
    "    print(final_output)\n",
    "\n",
    "# Run the main function if the script is executed directly\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
